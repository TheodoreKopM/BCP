{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "239bac19-8a51-40d8-80a0-a00872033b74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -qqqq -U pypdf==4.1.0 databricks-vectorsearch transformers==4.41.1 torch==2.3.0 tiktoken==0.7.0 langchain-text-splitters==0.2.2 mlflow mlflow-skinny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffd2a47e-ea69-42f1-8f55-7ffd2f0b2cd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, expr, current_timestamp, md5, explode, udf\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "import mlflow\n",
    "from mlflow.utils import databricks_utils as du\n",
    "from dlt import *\n",
    "import yaml\n",
    "from pypdf import PdfReader\n",
    "import io\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoTokenizer\n",
    "import tiktoken\n",
    "\n",
    "# Load configurations from YAML files\n",
    "def load_config(config_file):\n",
    "    with open(config_file, 'r') as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "# Load configurations\n",
    "data_pipeline_config = load_config('data_pipeline_config.yaml')\n",
    "destination_tables_config = load_config('destination_tables_config.yaml')\n",
    "\n",
    "# Extract configuration values\n",
    "UC_CATALOG = \"theodore_kop_personal\"  # This could also be loaded from a config file\n",
    "UC_SCHEMA = \"bcp\"\n",
    "RAG_APP_NAME = \"bcp_document_rag_poc\"\n",
    "\n",
    "# Source path for documents\n",
    "SOURCE_PATH = f\"/Volumes/theodore_kop_personal/bcp/rag_documentation\"\n",
    "\n",
    "# Define PDF parsing UDF\n",
    "def parse_pdf_udf(content):\n",
    "    \"\"\"Parse PDF content and return the extracted text.\"\"\"\n",
    "    try:\n",
    "        pdf = io.BytesIO(content)\n",
    "        reader = PdfReader(pdf)\n",
    "        parsed_content = [page.extract_text() for page in reader.pages]\n",
    "        return \"\\n\".join(parsed_content)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing PDF: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Register the PDF parsing UDF\n",
    "parse_pdf = udf(parse_pdf_udf, StringType())\n",
    "\n",
    "# Define text chunking UDF\n",
    "def chunk_text_udf(text):\n",
    "    \"\"\"Chunk text using the configured chunking strategy.\"\"\"\n",
    "    try:\n",
    "        embedding_config = data_pipeline_config[\"embedding_config\"]\n",
    "        chunker_config = data_pipeline_config[\"pipeline_config\"][\"chunker\"][\"config\"]\n",
    "        \n",
    "        # Select the correct tokenizer based on the embedding model configuration\n",
    "        if embedding_config[\"embedding_tokenizer\"][\"tokenizer_source\"] == \"hugging_faceXX\":\n",
    "            tokenizer = AutoTokenizer.from_pretrained(\n",
    "                embedding_config[\"embedding_tokenizer\"][\"tokenizer_model_name\"]\n",
    "            )\n",
    "            text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "                tokenizer,\n",
    "                chunk_size=chunker_config[\"chunk_size_tokens\"],\n",
    "                chunk_overlap=chunker_config[\"chunk_overlap_tokens\"],\n",
    "            )\n",
    "        elif embedding_config[\"embedding_tokenizer\"][\"tokenizer_source\"] == \"tiktoken\":\n",
    "            tokenizer = tiktoken.encoding_for_model(\n",
    "                embedding_config[\"embedding_tokenizer\"][\"tokenizer_model_name\"]\n",
    "            )\n",
    "            text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "                tokenizer,\n",
    "                chunk_size=chunker_config[\"chunk_size_tokens\"],\n",
    "                chunk_overlap=chunker_config[\"chunk_overlap_tokens\"],\n",
    "            )\n",
    "        else:\n",
    "            text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=chunker_config[\"chunk_size_tokens\"],\n",
    "                chunk_overlap=chunker_config[\"chunk_overlap_tokens\"],\n",
    "            )\n",
    "        \n",
    "        chunks = text_splitter.split_text(text)\n",
    "        return chunks\n",
    "    except Exception as e:\n",
    "        print(f\"Error chunking text: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "# Register the text chunking UDF\n",
    "chunk_text = udf(chunk_text_udf, ArrayType(StringType()))\n",
    "\n",
    "# Define the raw files table (Bronze)\n",
    "@dlt.table(\n",
    "    name=\"raw_files\",\n",
    "    comment=\"Raw files from UC Volume\",\n",
    "    table_properties={\n",
    "        \"quality\": \"bronze\",\n",
    "        \"pipelines.autoOptimize.zOrderCols\": \"path\"\n",
    "    }\n",
    ")\n",
    "def raw_files():\n",
    "    return (\n",
    "        spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"binaryFile\")\n",
    "        .option(\"cloudFiles.schemaLocation\", f\"/tmp/{RAG_APP_NAME}/schema\")\n",
    "        .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "        .option(\"recursiveFileLookup\", \"true\")\n",
    "        .option(\"pathGlobFilter\", \"*.pdf\")\n",
    "        .load(SOURCE_PATH)\n",
    "        .withColumn(\"processing_timestamp\", current_timestamp())\n",
    "    )\n",
    "\n",
    "# Define the parsed documents table (Silver)\n",
    "@dlt.table(\n",
    "    name=\"parsed_docs\",\n",
    "    comment=\"Parsed document contents\",\n",
    "    table_properties={\n",
    "        \"quality\": \"silver\",\n",
    "        \"pipelines.autoOptimize.zOrderCols\": \"path\"\n",
    "    }\n",
    ")\n",
    "@dlt.expect(\"valid_content\", \"parsed_content IS NOT NULL\")\n",
    "@dlt.expect(\"valid_path\", \"path IS NOT NULL\")\n",
    "def parsed_docs():\n",
    "    return (\n",
    "        dlt.read_stream(\"raw_files\")\n",
    "        .withColumn(\"parsed_content\", parse_pdf(\"content\"))\n",
    "        .select(\n",
    "            \"path\",\n",
    "            \"parsed_content\",\n",
    "            \"processing_timestamp\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Define the chunked documents table (Gold)\n",
    "@dlt.table(\n",
    "    name=\"chunked_docs\",\n",
    "    comment=\"Chunked document contents for vector search\",\n",
    "    table_properties={\n",
    "        \"quality\": \"gold\",\n",
    "        \"pipelines.autoOptimize.zOrderCols\": \"chunk_id\",\n",
    "        \"pipelines.autoOptimize.optimizeWrite\": \"true\",\n",
    "        \"delta.enableChangeDataFeed\" : \"true\"\n",
    "    }\n",
    ")\n",
    "@dlt.expect(\"valid_chunk\", \"chunked_text IS NOT NULL\")\n",
    "@dlt.expect(\"valid_chunk_id\", \"chunk_id IS NOT NULL\")\n",
    "def chunked_docs():\n",
    "    return (\n",
    "        dlt.read_stream(\"parsed_docs\")\n",
    "        .withColumn(\"chunks\", chunk_text(\"parsed_content\"))\n",
    "        .select(\n",
    "            \"path\",\n",
    "            explode(\"chunks\").alias(\"chunked_text\"),\n",
    "            \"processing_timestamp\"\n",
    "        )\n",
    "        .withColumn(\"chunk_id\", md5(\"chunked_text\"))\n",
    "    )\n",
    "\n",
    "# Define the data quality metrics\n",
    "@dlt.view(\n",
    "    name=\"quality_metrics\",\n",
    "    comment=\"Data quality metrics for the pipeline\"\n",
    ")\n",
    "def quality_metrics():\n",
    "    return spark.sql(\"\"\"\n",
    "        SELECT\n",
    "            'raw_files' as table_name,\n",
    "            COUNT(*) as total_records,\n",
    "            COUNT(CASE WHEN content IS NULL THEN 1 END) as null_content_count\n",
    "        FROM raw_files\n",
    "        UNION ALL\n",
    "        SELECT\n",
    "            'parsed_docs' as table_name,\n",
    "            COUNT(*) as total_records,\n",
    "            COUNT(CASE WHEN parsed_content IS NULL THEN 1 END) as null_parsed_content_count\n",
    "        FROM parsed_docs\n",
    "        UNION ALL\n",
    "        SELECT\n",
    "            'chunked_docs' as table_name,\n",
    "            COUNT(*) as total_records,\n",
    "            COUNT(CASE WHEN chunked_text IS NULL THEN 1 END) as null_chunked_text_count\n",
    "        FROM chunked_docs\n",
    "    \"\"\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "dependencies": [
     "pypdf",
     "langchain_text_splitters",
     "tiktoken"
    ],
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "streaming_document_pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
